<!doctype html>
<html>

    <head>
        <title>ğŸ‘ğŸ½Turing Complete User (reinterpreted)ğŸ‘ğŸ½</title>
        <link rel="stylesheet" href="css/alisStyle.css" /> 

    </head>
    <body>


        <!--

            


            in VSC:
            CMD+S or CTRL+S

            CMD+TAB or ALT+TAB

            in CHROME:
            CMD+R  or CTRL+R

            CMD+C/X or CtRL+C/X
            CMD+P or CtRL+P


            
            



        -->

            <a href="home.html">visit my home on the web</a>


            <div id="articleContainer">

    <h1>Turing Complete User</h1>
 
    <p>
â€œAny error may vitiate the entire output of the device. For the recognition and correction of such malfunctions intelligent human intervention will in general be necessary.â€
<br />
â€” John von Neumann, First Draft of a Report on the EDVAC, 1945
    </p>


    <p>
â€œIf you canâ€™t blog, tweet! If you canâ€™t tweet, like!â€
<br />
â€” Kim Dotcom, Mr. President, 2012
    </p>

    <ol>
    <li><a href="#invisible_and_very_busy">Invisible and Very Busy</a></li>
    <li><a href="#general_purpose_stupid_and_universal">General Purpose, â€œStupidâ€ and Universal</a></li>
    </ol>


<h3 id="invisible_and_very_busy">Invisible and Very Busy</h3>

<p>
Computers are getting invisible. They shrink and hide. They lurk under the skin and dissolve in the cloud. We observe the process like an eclipse of the sun, partly scared, partly overwhelmed. We divide into camps and fight about advantages and dangers of The Ubiquitous. But whatever side we take â€” we do acknowledge the significance of the moment.
</p>

<p>
With the disappearance of the computer, something else is silently becoming invisible as well â€” the User. Users are disappearing as both phenomena and term, and this development is either unnoticed or accepted as progress â€” an evolutionary step.
</p>

<p>
The notion of the Invisible User is pushed by influential user interface designers, specifically by Don Norman a guru of user friendly design and long time advocate of invisible computing. He can be actually called the father of Invisible Computing.
</p>

<p>
Those who study interaction design read his â€œWhy Interfaces Donâ€™t Workâ€ published in 1990 in which he asked and answered his own question: â€œThe real problem with the interface is that it is an interfaceâ€. Whatâ€™s to be done? â€œWe need to aid the task, not the interface to the task. The computer of the future should be invisible!â€
</p>

<p>
It took almost two decades, but the future arrived around five years ago, when clicking mouse buttons ceased to be our main input method and touch and multi-touch technologies hinted at our new emancipation from hardware. The cosiness of iProducts, as well as breakthroughs in Augmented Reality (it got mobile), rise of wearables, maturing of all sorts of tracking (motion, face) and the advancement of projection technologies erased the visible border between input and output devices. These developments began to turn our interactions with computers into pre-computer actions or, as interface designers prefer to say, â€œnaturalâ€ gestures and movements.
</p>
Of course computers are still distinguishable and locatable, but they are no longer something you sit in front of. The forecasts for invisibility are so optimistic that in 2012 Apple allowed to themselves to rephrase Normanâ€™s predictive statement by putting it in the present tense and binding it to a particular piece of consumer electronics:
</p>

<p>
We believe that technology is at its very best when it is invisible, when you are conscious only of what you are doing, not the device you are doing it with [â€¦] iPad is the perfect expression of that idea, itâ€™s just this magical pane of glass that can become anything you want it to be. Itâ€™s a more personal experience with technology than people have ever had.
</p>

<p>
In this last sentence, the word â€œexperienceâ€ is not an accident, neither is the word â€œpeopleâ€.
</p>

<p>
Invisible computers, or more accurately the illusion of the computerless, is destroyed if we continue to talk about â€œuser interfacesâ€. This is why Interface Design starts to rename itself to Experience Design â€” whose primary goal is to make users forget that computers and interfaces exist. With Experience Design there is only you and your emotions to feel, goals to achieve, tasks to complete.
</p>

<p>
The field is abbreviated as UXD, where X is for eXperience and U is still for the Users. Wikipedia says Don Norman coined the term UX in 1995. However, in 2012 UX designers avoid to use the U-word in papers and conference announcements, in order not to remind themselves about all those clumsy buttons and input devices of the past. Users were for the interfaces. Experiences, they are for the PEOPLE!
</p>

<p>
In 2008 Don Norman simply ceased to address Users as Users. At an event sponsored by Adaptive Path, a user interface design company, Norman stated â€œOne of the horrible words we use is users. I am on a crusade to get rid of the word â€˜usersâ€™. I would prefer to call them â€˜people.â€™â€ After enjoying the effect of his words on the audience he added with a charming smile, â€œWe design for people, we donâ€™t design for users.â€
</p>

<p>
A noble goal in deed, but only when perceived in the narrow context of Interface Design. Here, the use of the term â€œpeopleâ€ emphasizes the need to follow the user centered in opposition to an implementation centered paradigm. The use of â€œpeopleâ€ in this context is a good way to remind software developers that the User is a human being and needs to be taken into account in design and validation processes.
</p>

<p>
But when you read it in a broader context, the denial of the word â€œuserâ€ in favor of â€œpeopleâ€ becomes dangerous. Being a User is the last reminder that there is, whether visible or not, a computer, a programmed system you use.
</p>

<p>
In 2011 new media theoretician Lev Manovich also became unhappy about the word â€œuserâ€. He writes on his blog â€œFor example, how do we call a person who is interacting with digital media? User? No good.â€
</p>

<p>
Well, I can agree that with all the great things we can do with new media â€” various modes of initiation and participation, multiple roles we can fill â€” that it is a pity to narrow it down to â€œusersâ€, but this is what it is. Bloggers, artists, podcasters and even trolls are still users of systems they didnâ€™t program. So they (we) are all the users.
</p>

<p>
We need to take care of this word because addressing people and not users hides the existence of two classes of people â€” developers and users. And if we lose this distinction, users may lose their rights and the opportunity to protect them. These rights are to demand better software, the ability â€œto choose none of the aboveâ€, to delete your files, to get your files back, to fail epically and, back to the fundamental one, to see the computer.
</p>

<p>
In other words: the Invisible User is more of an issue than an Invisible Computer.
</p>

<p>
What can be done to protect the term, the notion and the existence of the Users? What counter arguments can I find to stop Normanâ€™s crusade and dispel Manovichâ€™s skepticism? What do we know about a user, apart from the opinion that it is â€œno goodâ€ to be one?
</p>

<p>
We know that it was not always like this. Before Real Users (those who pay money to use the system) became â€œusersâ€, programmers and hackers proudly used this word to describe themselves. In their view, the user was the best role one could take in relation to their computer.
</p>

<p>
Furthermore, it is wrong to think that first there were computers and developers and only later users entered the scene. In fact, it was the opposite. At the dawn of personal computer the user was the center of attention. The user did not develop in parallel with the computer, but prior to it. Think about Vanevar Bushâ€™s â€œAs we May Thinkâ€ (1945), one of the most influential texts in computer culture. Bush spends more words describing the person who would use the Memex than the Memex itself. He described a scientists of the future, a superman. He, the user of the Memex, not the Memex, itself was heading the article.
</p>

<p>
20 years later, Douglas Engelbart, inventor of the pioneering personal computer system NLS, as well as hypertext, and the mouse, talked about his research on the augmentation of human intellect as â€œbootstrapingâ€ â€” meaning that human beings, and their brains and bodies, will evolve along with new technology. This is how French sociologist Thierry Bardini describes this approach in his book about Douglas Engelbart: â€œEngelbart wasnâ€™t interested in just building the personal computer. He was interested in building the person who could use the computer to manage increasing complexity efficiently.â€
</p>

<p>
And letâ€™s not forget the title of J.C.R. Lickliderâ€™s famous text, the one that outlined the principles for APRAs Command and Control research on Real Time System, from which the interactive/personal computer developed â€” Man-Computer Symbiosis (1960).
</p>

<p>
When the personal computer was getting ready to enter the market 15 years later, developers thought about who would be model users. At XEROX PARC, Alan Kay and Adele Goldberg introduced the idea of kids, artists, musicians and others as potential users for the new technology. Their paper â€œPersonal Dynamic Mediaâ€ from 1977 describes important hardware and software principles for the personal computer. But we read this text as revolutionary because it clearly establishes possible users, distinct from system developers, as essential to these dynamic technologies. Another Xerox employee, Tim Mott (aka â€œThe father of user centered designâ€) brought the idea of a Secretary into the imagination of his colleagues. This image of the â€œLady with the Royal Typewriterâ€ predetermined the designs of XEROX Star, Apple Lisa and and further electronic offices.
</p>

<p>
So, itâ€™s important to acknowledge that users existed prior to computers, that they were imagined and invented â€” Users are the figment of the imagination. As a result of their fictive construction, they continued to be re-imagined and re-invented through the 70â€™s, 80â€™s, 90â€™s, and the new millennium. But however reasonable, or brave, or futuristic, or primitive these models of users were, there is a constant.</p>

<p>
    Let me refer to another guru of user centered design, Alan Cooper. In 2007, when the U word was still allowed in interaction design circles, he and his colleagues shared their secret in â€œAbout Face, The Essentials of Interaction Designâ€:
</p>

<p>
    â€œAs an interaction designer, itâ€™s best to imagine that users â€“ especially â€” beginners â€” are simultaneously very intelligent and very busy.â€
</p>

<p>
    It is very kind advice (and one of the most reasonable books on interface design, btw) and can be translated roughly as â€œhey, front end developers, donâ€™t assume that your users are more stupid than you, they are just busy.â€ But it is more than this. What the second part of this quote gets to so importantly is that Users are people who are very busy with something else.
</p>

<p>
    Alan Cooper is not the one who invented this paradigm, and not even Don Norman with his concentration on task rather than the tool. It originated in the 1970â€™s. Listing the most important computer terms of that time, Ted Nelson mentions so called â€œuser level systemsâ€ and states that these â€œUser-level systems, [are] systems set up for people who are not thinking about computers but about the subject or activity the computer is supposed to help them with. Some pages before he claims:
</p>

<img src="http://contemporary-home-computing.org/turing-complete-user/personal-computing.png" />


<p>
    One should remember that Ted Nelson was always on the side of users and even â€œnaÃ¯ve usersâ€ so his bitter â€œjust a userâ€ means a 
    lot.
</p>

<p>
Alienation of users from their computers started in XEROX PARC with secretaries, as well as artists and musicians. And it never stopped. Users were seen and marketed as people whoâ€™s real jobs, feelings, thoughts, interests, talents â€” everything what matters â€” lie outside of their interaction with personal computers.
</p>

<p>

For instance, in 2007, when Adobe, the software company whoâ€™s products are dominating the so called â€œcreative industriesâ€, introduced version 3 of Creative Suite, they filmed graphic artists, video makers and others talking about the advantages of this new software package. In particular interesting was one video of a web designer (or an actress in the role of a web designer): she enthusiastically demonstrated what her new Dream Weaver could do, and that in the end â€œI have more time to do what I like most â€” being creativeâ€. The message from Adobe is clear. The less you think about source code, scripts, links and the web itself, the more creative you are as a web designer. What a lie. I liked to show it to fresh design students as an example of misunderstanding the core of the profession.
</p>

<p>

This video is not online anymore, but actual ads for Creative Suite 6 are not much different â€“ they feature designers and design evangelists talking about unleashing, increasing and enriching creativity as a direct result of fewer clicks to achieve this or that effect.
</p>

<p>

In the book â€œProgram or be Programmedâ€, Douglas Rushkoff describes similar phenomena:
</p>

<p>

[â€¦] We see actual coding as some boring chore, a working class skill like bricklaying, which may as well be outsourced to some poor nation while our kids play and even design video games. We look at developing the plots and characters for a game as the interesting part, and the programming as the rote task better offloaded to people somewhere else.17
</p>

<p>

Rushkoff states that code writing is not seen as a creative activity, but the same applies to engagement with the computer in general. It is not seen as a creative task or as â€œmature thoughtâ€.
</p>

<p>

In â€œAs we may thinkâ€, while describing an ideal instrument that would augment the scientist of the future, Vanevar Bush mentions
</p>

<p>

For mature thought there is no mechanical substitute. But creative thought and essentially repetitive thought are very different things. For the latter there are, and may be, powerful mechanical aids
</p>

<p>

Opposed to this, users, as imagined by computer scientists, software developers and usability experts are the ones whoâ€™s task is to spend as little time as possible with the computer, without wasting a single thought on it. They require a specialized, isolated app for every â€œrepetitive thoughtâ€, and, most importantly, delegate drawing the border in between creative and repetitive, mature and primitive, real and virtual, to app designers.
</p>

<p>

There are periods in history, moments in life (and many hours a day!) where this approach makes sense, when delegation and automation are required and enjoyed. But in times when every aspect of life is computerized it is not possible to accept â€œbusy with something elseâ€ as a norm.
</p>

<p>

So letâ€™s look at another model of users that evolved outside and despite usability expertsâ€™ imagination.

</p>


<img src="http://contemporary-home-computing.org/turing-complete-user/scientist.jpg" />
<p class="caption">
    <em>â€œA scientist of the Futureâ€</em><br />
Title picture of Vanevar Bushâ€™s â€œAs we make thinkâ€<br />
Illustrated version from Life magazine, 1945
</p>

<img src="http://contemporary-home-computing.org/turing-complete-user/sergey-dolya.jpg" />
<p class="caption">
Russian travel blogger Sergey Dolya<br />
photo by Mik Sazonov, 2012
</p>
<h3 id="general_purpose_stupid_and_universal">General Purpose, â€œStupidâ€ and Universal</h3>
<p>
In â€œWhy Interfaces Donâ€™t Workâ€ Don Norman heavily criticizes the world of visible computers, visible interfaces and users busy with all this. Near the end of the text he suggests the source of the problem:
</p>

<p>

</p>

<p>
â€œWe are here in part, because this is probably the best we can do with todayâ€™s technology and, in part, because of historical accident. The accident is that we have adapted a general-purpose technology to very specialized tasks while still using general tools.â€19

</p>

<p>
In December 2011 science fiction writer and journalist Cory Doctorow gave a marvelous talk at the 28th Chaos Communication Congress in Berlin titled â€œThe coming war on general computationâ€. He explains that there is only one possibility for computers to truly become appliances, the tiny, invisible, comfortable one purpose things Don Norman was preaching about: to be loaded with spyware. He explains,

</p>

<p>
â€œSo today we have marketing departments who say things like â€˜[â€¦] Make me a computer that doesnâ€™t run every program, just a program that does this specialized task, like streaming audio, or routing packets, or playing Xbox gamesâ€™ [â€¦] But thatâ€™s not what we do when we turn a computer into an appliance. Weâ€™re not making a computer that runs only the â€œapplianceâ€ app; weâ€™re making a computer that can run every program, but which uses some combination of rootkits, spyware, and code-signing to prevent the user from knowing which processes are running, from installing her own software, and from terminating processes that she doesnâ€™t want. In other words, an appliance is not a stripped-down computer â€” it is a fully functional computer with spyware on it out of the box.â€

</p>

<p>
By fully functional computer Doctorow means the general purpose computer, or as US mathematician John von Neumann referred to it in his 1945 â€œFirst Draft of a Report on the EDVACâ€ â€” the â€œall purpose automatic digital computing systemâ€. In this paper he outlined the principles of digital computer architecture (von Neumann Architecture), where hardware was separated from the software and from this the so called â€œstored programâ€ concept was born. In the mid 40â€™s the revolutionary impact of it was that â€œby storing the instructions electronically, you could change the function of the computer without having to change the wiring.â€

</p>

<p>
Today the rewiring aspect doesnâ€™t have to be emphasized, but the idea itself that a single computer can do everything is essential, and that it is the same general purpose computer behind â€œeverythingâ€ from dumb terminals to super computers.

</p>

<p>
Doctorowâ€™s talk is a perfect entry point to get oneself acquainted with the subject. To go deeper into the history of the war on general computation you may consider reading Ted Nelson. He was the first to attract attention to the significance of the personal computerâ€™s all-purpose nature. In 1974 in his glorious fanzine â€œComputer Libâ€ which aimed to explain computers to everybody, he writes in caps lock:
</p>
<p>COMPUTERS HAVE NO NATURE AND NO CHARACTER</p>
<p>
Computers are, unlike any other piece of equipment, perfectly BLANK. And that is how we have projected on it so many different faces.
</p>

<p>

Some great texts written this century are â€œThe Future of the Internet and How to Stop Itâ€ (2008) by Jonathan Zittrain and of course â€œThe Future of Ideasâ€ (2001) by Lawrence Lessig. Both authors are more concerned with the architecture of the internet than the computer itself but both write about the end-to-end principle that lies at the internetâ€™s core â€” meaning that there is no intelligence (control) build into the network. The network stays neutral or â€œstupidâ€, simply delivering packets without asking whatâ€™s inside. It is the same with the von Neuman computer â€” it just runs programs.
</p>

<p>
The works of Lessig, Zittrain and Doctorow do a great job of explaining why both computer and network architectures are neither historic accidents nor â€œwhat technology wantsâ€.24 The stupid network and the general purpose computer were conscious design decisions.
</p>

<p>
For Norman, further generations of hardware and software designers and their invisible users dealing with General Purpose technology is both accident and obstacle. For the rest of us the rise and use of General Purpose Technology is the core of New media, Digital Culture and Information Society (if you believe that something like this exists). General purpose computers and Stupid Networks are the core values of our computer-based time and the driving force behind all the wonderful and terrible things that happen to people who work and live with connected computers. These prescient design decisions have to be protected today, because technically it would be no big deal to make networks and computers â€œsmartâ€, i.e. controlled.
</p>

<p>
What does it all have to do with â€œusersâ€ versus â€œpeopleâ€ â€” apart from the self evident fact that only the users who are busy with computers at least a little bit â€” to the extent of watching Doctorowâ€™s video till the end â€” will fight for these values?
</p>

<p>
I would like to apply the concept of General Purpose Technology to users by flipping the discourse around and redirecting attention from technology to the user that was formed through three decades of adjusting general purpose technology to their needs: The General Purpose User.
</p>

<p>
General Purpose Users can write an article in their e-mail client, layout their business card in Excel and shave in front of a web cam. They can also find a way to publish photos online without flickr, tweet without twitter, like without facebook, make a black frame around pictures without instagram, remove a black frame from an instagram picture and even wake up at 7:00 without a â€œwake up at 7:00â€ app.
</p>

<p>
Maybe these Users could more accurately be called Universal Users or Turing Complete Users, as a reference to the Universal Machine, also known as Universal Turing Machine â€” Alan Turingâ€™s conception of a computer that can solve any logical task given enough time and memory. Turingâ€™s 1936 vision and design predated and most likely influenced von Neumanâ€™s First Draft and All-purpose Machine.
</p>

<p>
But whatever name I chose, what I mean are users who have the ability to achieve their goals regardless of the primary purpose of an application or device. Such users will find a way to their aspiration without an app or utility programmed specifically for it. The Universal user is not a super user, not half a hacker. It is not an exotic type of user.
</p>

<p>
There can be different examples and levels of autonomy that users can imagine for themselves, but the capacity to be universal is still in all of us. Sometimes it is a conscious choice not to delegate particular jobs to the computer, and sometimes it is just a habit. Most often it is not more than a click or two that uncover your general purpose architecture.
</p>

<p>
For instance, you can decide not to use Twitter at all and instead inform the world about your breakfast through your own website. You can use Live Journal as if it is Twitter, you can use Twitter as Twitter, but instead of following people, visit their profiles as youâ€™d visit a homepage.
</p>

<p>
You can have two Twitter accounts and log in to one in Firefox, and the other in Chrome. This is how I do it and it doesnâ€™t matter why I prefer to manage it this way. Maybe I donâ€™t know that an app for managing multiple accounts exists, maybe I knew but didnâ€™t like it, or maybe Iâ€™m too lazy to install it. Whatever, I found a way. And you will do as well.
</p>

<p>
A Universal Userâ€™s mind set (it is a mind set, not set of rules, not a vow) means to liaise with hardware and software. Behavior that is antipodal to the â€œvery busyâ€ user. This kind of interaction makes the user visible, most importantly to themselves. And, if you wish to think about it in terms of Interface Design and UX, it is the ultimate experience.
</p>

<p>
Does this mean that to deliver this kind of user experience the software industry needs to produce imperfect software or hold itself back from improving existing tools? Of course not! Tools can be perfect.
</p>

<p>
Though the idea of perfect software could be revised, taking into account that it is used by the General Purpose User, valuing ambiguity and usersâ€™ involvement.
</p>

<p>
And thankfully ambiguity is not that rare. There are online services where users are left alone to use or ignore features. For example, the developers of Twitter didnâ€™t take measures that prevent me from surfing from profile to profile of people I donâ€™t follow. The Dutch social network Hyves allows their users to mess around with background images so that they donâ€™t need any photo albums or instagrams to be happy. Blingee.com, whoâ€™s primary goal is to let users add glitter to their photos, allows to upload whatever stamps they want â€” not glittery, not even animated. It just delivers the user merged layers in return.
</p>

<p>
I can also mention here an extreme example of a service that nourishes the userâ€™s universality â€” myknet.org â€” an Aboriginal social network in Canada. It is so â€œstupidâ€ that users can re-purpose their profiles every time they update them. Today it functions as a twitter feed, yesterday it was a youtube channel, and tomorrow it might be an online shop. Never-mind that it looks very low-tech and like it was made years ago, it works!
</p>

<p>
In general the WWW, outside of Facebook, is an environment open for interpretation.
</p>

<p>
Still, I have difficulties finding a site or an app, that actually addresses the users, and sees their presence as a part of the work flow. This maybe sounds strange, because all web 2.0 is about pushing people to contribute, and â€œemotional designâ€ is supposed to be about establishing personal connections in between people who made the app and people who bought it, but I mean something different. I mean a situation when the work flow of an application has gaps that can be filled by users, where smoothness and seamlessness are broken and some of the final links in the chain are left for the users to complete.
</p>

<p>
Iâ€™ll leave you with an extreme example, an anonymous (probably student) project:


â€œGoogle Maps + Google Video + Mashup â€” Claude Lelouchâ€™s Rendezvousâ€:

<img src="http://contemporary-home-computing.org/turing-complete-user/mashup.png" />

<p>
It was made in 2006, at the very rise of Web 2.0, when the mash-up was a very popular cultural, mainstream artistic form. Artists were celebrating new convergences and a blurring of the borders between different pieces of software. Lelouchâ€™s Rendezvous is a mash up that puts on the same page the famous racing film of the same name and a map of Paris, so that you can follow the car in the film and see its position on the Google map at the same time. But the author failed (or perhaps didnâ€™t intend) to synchronize the video and the carâ€™s movement on the map. As a result the user is left with the instruction: â€œHit play on the video. [â€¦] At the 4 second mark, hit the â€˜Go!â€™ button.â€
</p>

<p>
The user is asked not only to press one but two buttons! It suggests that we take care ourselves, that we make can complete a task at the right moment. The author obviously counts on users intelligence, and never heard that they are â€œvery busyâ€.
</p>

<p>
The fact that the original video file that was used in the mash up was removed, makes this project even more interesting. To enjoy it, youâ€™ll have to go to YouTube and look for another version of the film. I found one, which means youâ€™ll succeed as well.
</p>

<p>
There is nothing one user can do, that another canâ€™t given enough time and respect. Computer Users are Turing Complete.
</p>

<p>
* * *
</p>

<p>
When Sherry Turkle, Douglas Rushkoff and other great minds state that we need to learn programming and understand our computers in order to not be programmed and â€œdemand transparency of other systemsâ€, I couldnâ€™t agree more. If the approach to computer education in schools was to switch from managing particular apps to writing apps it will be wonderful. But apart from the fact that it is not realistic, I would say it is also not enough. I would say it is wrong to say either you understand computers or u are the user.
</p>

<p>
An effort must be made to educate the users about themselves. There should be understanding of what it means to be a user of an â€œall purpose automatic digital computing systemâ€.
</p>

<p>
General Purpose Users are not a historic accident or a temporary anomaly. We are the product of the â€œworse is betterâ€ philosophy of UNIX, the end-to end principle of the internet, the â€œunder constructionâ€ and later â€œbetaâ€ spirit of the web. All these designs that demand attention, and ask for forgiveness and engagement formed us as users, and we are always adjusting, improvising and at the same time taking control. We are the children of the misleading and clumsy Desktop Metaphor, we know how to open doors without knobs.
</p>

<p>
We, general purpose users â€” not hackers and not people â€” who are challenging, consciously or subconsciously, what we can do and what computers can do, are the ultimate participants of man-computer symbiosis. Not exactly the kind of symbiosis Licklider envisioned, but a true one.
</p>

<p>
Olia Lialina, October 2012
</p>

<p>
    <h4><a href="http://contemporary-home-computing.org/turing-complete-user/" target="_blank">click here for the original text!</a></h4>

</p>
    </div>
    </body>
</html>